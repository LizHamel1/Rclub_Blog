---
title: "DREAM Mixture ML Intro"
author: "Joel Mainland"
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
  html_document:
    number_sections: false
    toc: true
    toc_float: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir="~/Documents/R/DREAM Mixtures")

if (!require("pacman")) install.packages("pacman")
#Load packages
pacman::p_load(tidyverse,randomForest,caret,GGally)

theme_set(theme_light())

#Note that I start with a brief slide show to re-introduce the challenge
```

# DREAM Olfactory Mixtures Challenge

The DREAM Olfactory Mixtures Challenge launched in April of 2024. 

In this post we will take a look at the training data for this challenge and do some very basic machine learning.

Recent advances in predictive methods and availability of perceptual data have paved the way for a growing interest in olfactory perception predictions from chemical representations of molecules. This has led to a growing consensus that for pure odors, it is possible to build models using the chemical structure of molecules to predict the perceptual values of natural language attributes of smells. However, predictions have mainly focused on pure molecules and not the real-world situation of olfactory mixtures. In order to start filling this gap, we plan to organize a second DREAM olfaction prediction challenge now focused on predicting the discriminability of olfactory mixtures. Using publicly available data from 3 different studies (Bushdid et al 2014, Snitz et al 2013, Ravia et al 2020) for more than 700 unique mixtures and almost 600 measurements of mixture pairs discriminability, participants will be tasked to predict the discriminability of 46 unpublished mixture pairs. 

You can register for the challenge here: https://www.synapse.org/Synapse:syn53470621/wiki/626022

## Look at the data

```{r}

#This is generated from Preprocessing.R
mixturesWithFeatures <- read.csv("data/processed/MixturesWithFeatures.csv")

# You can load from Dropbox if you are having issues loading from the workspace:
# mixturesWithFeatures <- read.csv("https://www.dropbox.com/scl/fi/f75j07xedtsv3774con0k/MixturesWithFeatures.csv?rlkey=lyfw541vdb6byhpj1341etvnu&dl=1", row.names=NULL)

glimpse(mixturesWithFeatures)

```



## Visualize the data

```{r}

mixturesWithFeatures %>%
  ggplot(aes(x = experimental_values)) +
  geom_histogram() +
  labs(x="Percentage discrimination")

```


High values on the x-axis correspond to mixture pairs that are easy to discriminate. There are a handful of mixture pairs that are very similar to each other.

```{r}
ggpairs(mixturesWithFeatures,columns=c(4,9,10,11))

```


```{r}

#Discrimination vs. Difference in size
mixturesWithFeatures %>%
  ggplot(aes(y = diff_mixture_size, x = experimental_values)) +
  geom_point() +
  geom_smooth(method = "lm")+
  labs(x="Percentage discrimination",y="Difference in size")

```


In general, mixture pairs that differ in the number of components are easier to discriminate. This effect appears to be driven by the very similar pairs that are all also very similar in size.

```{r}
#Discrimination vs. Percentage overlap
mixturesWithFeatures %>%
  ggplot(aes(y = overlap_percent, x = experimental_values)) +
  geom_point() +
  geom_smooth(method = "lm")+
  labs(x="Percentage discrimination",y="Percentage overlap")
```


In general, pairs with low overlap in components are easy to discriminate.

We can see one mixture pair that has no overlap in molecules, but could only be discriminated on ~15% of trials. This is a possible metameric pair.

Snitz et al., 2013 has a published algorithm for predicting how similar two mixtures are, and their data are in this training set. Let's see how well that model does on all of the data.

```{r}
#Discrimination vs. Angle Distance
mixturesWithFeatures %>%
  ggplot(aes(y = angle_dist, x = experimental_values)) +
  geom_point() +
  geom_smooth(method = "lm")+
  labs(x="Percentage discrimination",y="Angle Distance")

fit1 <- lm(angle_dist ~ experimental_values, data = mixturesWithFeatures)

print(paste0("Adj R2 = ",signif(summary(fit1)$adj.r.squared, 5)))
print(paste0(" P =",signif(summary(fit1)$coef[2,4], 5)))

```


## Basic machine learning

### Split the data
```{r}
mixtures.clean <- mixturesWithFeatures %>% 
  #mutate(ID = row_number()) %>%
  select(experimental_values,diff_mixture_size,overlap_percent,angle_dist)

#divide into training and test sets
set.seed(42)
# Create a data partition: 80% for training, 20% for testing
trainIndex <- createDataPartition(mixtures.clean$experimental_values, p = 0.8, list = FALSE)

# Create the training and testing sets
train_set <- mixtures.clean[trainIndex, ]
test_set <- mixtures.clean[-trainIndex, ]
```

### Fit a linear model
```{r}
#fit a linear model
model_linear <- lm(experimental_values ~ diff_mixture_size + overlap_percent + angle_dist, data = train_set)

# View the summary of the model
summary(model_linear)

```

### Make predictions
```{r}
test_set$predicted <- predict(model_linear, newdata = test_set)

test_set %>% 
  ggplot(aes(x=experimental_values,y=predicted))+
  geom_point() +
  geom_smooth(method = "lm")+
  labs(x="Percentage discrimination",y="Predicted")

```

### Calculate metrics
```{r}
# Calculate Mean Squared Error
mse <- mean((test_set$experimental_values - test_set$predicted)^2)
print(paste("Mean Squared Error:", mse))

# Calculate R-squared on the test set
ss_total <- sum((test_set$experimental_values - mean(test_set$experimental_values))^2)
ss_residual <- sum((test_set$experimental_values - test_set$predicted)^2)
r_squared <- 1 - (ss_residual / ss_total)
print(paste("R-squared on test set:", r_squared))
```


### Random Forest

```{r}

#Build a random forest
r = randomForest(experimental_values ~., data=train_set, importance=TRUE, do.trace=100)
print(r)

```
Tree shows the number of trees at each stage of evaluation.
MSE is the mean-squared error of the predictions for out-of-bag samples.
Percentage of variance explained--higher is better.

MSE decreases slightly as we increase the number of trees, but variance explained slightly decreases. The model stablilizes around 300 trees.

Note that the final variance explained is much lower than the estimates. This is likely because we are overfitting with only three variables.

### Try the random forest model on the test set
```{r}
#Now try it on the test set
mixture.predict = predict(r, test_set)
mixture.results <- cbind(test_set,Predicted=mixture.predict)

mixture.results %>%
  ggplot(aes(y = mixture.predict, x = experimental_values)) +
  geom_point() +
  geom_smooth(method = "lm")+
  labs(x="Percentage discrimination",y="Random Forest Model")

fit2 <- lm(mixture.predict ~ experimental_values, data = mixture.results)

print(paste0("Adj R2 = ",signif(summary(fit2)$adj.r.squared, 5)))
print(paste0(" P =",signif(summary(fit2)$coef[2,4], 5)))

```

### Look at which variables are most important
```{r}
importance(r)
```

%IncMSE: This column shows the percentage increase in the mean squared error (MSE) when a given variable is randomly permuted (its values are shuffled). A higher value indicates that the variable is more important for predicting the target variable because permuting it leads to a larger increase in the MSE.

IncNodePurity: This column shows the total decrease in node impurity (measured by residual sum of squares for regression) that results from splits on this variable, averaged over all trees. A higher value indicates that the variable contributes more to reducing the impurity of nodes in the trees, thus making it a more important predictor.



```{r}
varImpPlot(r)
```

Difference in mixture size is the least important variable. The two metrics disagree on which of the other two predictors are more important

